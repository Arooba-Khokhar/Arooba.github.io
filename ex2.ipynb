{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 2: Network Regularization (CNN).\n",
    "I will add dropout to the training.You have to compare both the solutions with and without dropout regularization\n",
    "\n",
    "#### First we load the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "import tarfile\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following function loads the data from the batch file and reshapes the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_cfar10_batch(batch_id):\n",
    "    with open('data_batch_' + str(batch_id), mode='rb') as file:\n",
    "        batch = pickle.load(file, encoding='latin1')\n",
    "        \n",
    "    features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    labels = batch['labels']\n",
    "        \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize function to normalize the values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "        argument\n",
    "            - x: input image data in numpy array [32, 32, 3]\n",
    "        return\n",
    "            - normalized x \n",
    "    \"\"\"\n",
    "    min_val = np.min(x)\n",
    "    max_val = np.max(x)\n",
    "    x = (x-min_val) / (max_val-min_val)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We perform one-hot encoding to the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "        argument\n",
    "            - x: a list of labels\n",
    "        return\n",
    "            - one hot encoding matrix (number of labels, number of class)\n",
    "    \"\"\"\n",
    "    encoded = np.zeros((len(x), 10))\n",
    "    \n",
    "    for idx, val in enumerate(x):\n",
    "        encoded[idx][val] = 1\n",
    "    \n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data\n",
    "\n",
    "#### The code cell below uses the previously implemented functions, normalize and one_hot_encode, to preprocess the given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode):\n",
    "    n_batches = 5\n",
    "    valid_features = []\n",
    "    valid_labels = []\n",
    "    \n",
    "    c10_train_dataset, c10_train_labels = [], []\n",
    "\n",
    "    for batch_i in range(1, n_batches + 1):\n",
    "        features, labels = load_cfar10_batch(batch_i)\n",
    "        \n",
    "        features = normalize(np.array(features))\n",
    "        labels = one_hot_encode(np.array(labels))\n",
    "    \n",
    "        c10_train_dataset.append(features)\n",
    "        c10_train_labels.append(labels)\n",
    "            \n",
    "        # find index to be the point as validation data in the whole dataset of the batch (10%)\n",
    "        index_of_validation = int(len(features) * 0.1)\n",
    "        \n",
    "        '''\n",
    "        valid_features.extend(features[-index_of_validation:])\n",
    "        valid_labels.extend(labels[-index_of_validation:])\n",
    "        '''\n",
    "\n",
    "    # load the test dataset\n",
    "    with open('test_batch', mode='rb') as file:\n",
    "        batch = pickle.load(file, encoding='latin1')\n",
    "\n",
    "    # preprocess the testing data\n",
    "    test_features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    test_labels = batch['labels']\n",
    "    \n",
    "    test_features = normalize(np.array(test_features))\n",
    "    test_labels = one_hot_encode(np.array(test_labels))\n",
    "    \n",
    "    return c10_train_dataset, c10_train_labels, test_features, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call the function and load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c10_train_dataset, c10_train_labels, c10_test_dataset,c10_test_labels = \\\n",
    "preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(c10_test_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the following cells we will setup our structure of CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = tf.placeholder(tf.float32, shape=(None, 32, 32, 3), name='input_x')\n",
    "y =  tf.placeholder(tf.float32, shape=(None, 10), name='output_y')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <<<  I used the ReLu activation function >>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def conv_net(x, keep_prob):\n",
    "    conv1_filter = tf.Variable(tf.truncated_normal(shape=[3, 3, 3, 64], mean=0, stddev=0.08))\n",
    "    bias1 = tf.Variable(tf.constant(0.05, shape=[64]))\n",
    "\n",
    "    conv1 = tf.nn.conv2d(x, conv1_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    conv1 += bias1\n",
    "    \n",
    "    conv1_pool = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    conv1 = tf.nn.relu(conv1_pool)\n",
    "    conv1_bn = tf.layers.batch_normalization(conv1_pool)\n",
    "\n",
    "    conv2_pool = tf.nn.max_pool(conv1_bn, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    \n",
    "    flat = tf.contrib.layers.flatten(conv1_bn)  \n",
    "\n",
    "    full1 = tf.contrib.layers.fully_connected(inputs=flat, num_outputs=64, activation_fn=tf.nn.relu)\n",
    "    full1 = tf.nn.dropout(full1, keep_prob)\n",
    "    \n",
    "    full2 = tf.contrib.layers.fully_connected(inputs=full1, num_outputs=256, activation_fn=tf.nn.relu)\n",
    "    full2 = tf.nn.dropout(full2, keep_prob)\n",
    "    \n",
    "    out = tf.contrib.layers.fully_connected(inputs=full2, num_outputs=10, activation_fn=tf.nn.softmax)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "* `epochs`: number of iterations until the network stops learning or start overfitting\n",
    "* `batch_size`: highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    "* `keep_probability`: probability of keeping a node using dropout\n",
    "* `learning_rate`: number how fast the model learns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LOGDIR = \"C:\\\\Users\\\\stech\\\\Downloads\\\\lab 6 (1)\\\\lab 6\\\\cifar-10-batches-py\\\\tensorboard\"\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "keep_probability = 0.9\n",
    "learning_rate = 0.001\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We initialize our CNN, define the optimizer and loss function and also accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = conv_net(x, keep_prob)\n",
    "model = tf.identity(logits, name='logits') # Name logits Tensor, so that can be loaded from disk after training\n",
    "\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy_train = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy_train')\n",
    "accuracy_test = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy_test')\n",
    "\n",
    "tf.summary.histogram('accuracy_train',accuracy_train)\n",
    "tf.summary.histogram('accuracy_test',accuracy_test)\n",
    "tf.summary.histogram(\"loss\", cost)\n",
    "\n",
    "tf.summary.scalar(\"loss\", cost)\n",
    "tf.summary.scalar(\"accuracy_train\", accuracy_train)\n",
    "tf.summary.scalar(\"accuracy_test\", accuracy_test)\n",
    "\n",
    "\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This function performs the learning/optimization using the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_neural_network(session, optimizer, feature_batch, label_batch):\n",
    "    session.run(optimizer, \n",
    "                feed_dict={\n",
    "                    x: feature_batch,\n",
    "                    y: label_batch,\n",
    "                    keep_prob: keep_probability\n",
    "                })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "\n",
    "#### The function print_stats runs the cost function. Accuracy function is also run on training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global count\n",
    "count = 0\n",
    "\n",
    "def print_stats(session, feature_batch, label_batch, cost, batch_features_test, \n",
    "                batch_labels_test, merged_op, sum_writer):\n",
    "    global count\n",
    "    loss = sess.run(cost, \n",
    "                    feed_dict={\n",
    "                        x: feature_batch,\n",
    "                        y: label_batch,\n",
    "                        keep_prob: 1.\n",
    "                    })\n",
    "    valid_acc = sess.run(accuracy_train, \n",
    "                         feed_dict={\n",
    "                             x: feature_batch,\n",
    "                             y: label_batch,\n",
    "                             #x: valid_features,\n",
    "                             #y: valid_labels,\n",
    "                             keep_prob: 1.\n",
    "                         })\n",
    "    \n",
    "    test_acc, summary_test = sess.run([accuracy_test,merged_op], feed_dict={x: batch_features_test, y: batch_labels_test, keep_prob: 1.})\n",
    "    \n",
    "    #train_writer.add_summary(summary_train, 1)\n",
    "    #test_writer.add_summary(summary_test, count)\n",
    "    sum_writer.add_summary(summary_test, count)\n",
    "    \n",
    "    count += 1\n",
    "    \n",
    "    print('Loss: {:>2.4f} , Training Accuracy: {:>2.6f} , Testing Accuracy: {:>2.6f}'.format(loss, valid_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Here\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "train_neural_network() takes 4 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-94075cf98587>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m                 \u001b[0mbatch_labels_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc10_test_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mincrement\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msubbatch_size\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mincrement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                 \u001b[0mtrain_neural_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_probability\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m                 print('Epoch # {}, CIFAR-10 Batch # {}, chunk = [{}:{}]  '.format(epoch + 1, batch_i,\n",
      "\u001b[1;31mTypeError\u001b[0m: train_neural_network() takes 4 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "save_model_path = './image_classification'\n",
    "\n",
    "tf.summary.FileWriterCache.clear()\n",
    "\n",
    "epochs = 50\n",
    "increment = 0\n",
    "subbatch_size = 50\n",
    "n_batches = 5\n",
    "\n",
    "num_steps = 200     # 20 #loop over data once\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    summary_writer = tf.summary.FileWriter(LOGDIR+'Q2', graph=tf.get_default_graph())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        #n_batches = 5\n",
    "        for batch_i in range(n_batches):\n",
    "            print('Here')\n",
    "            increment = 0\n",
    "            for sub_batch in range(num_steps):\n",
    "                batch_features = c10_train_dataset[batch_i][increment : (subbatch_size + increment)]\n",
    "                batch_labels = c10_train_labels[batch_i][increment : (subbatch_size + increment)]\n",
    "            \n",
    "                batch_features_test = c10_test_dataset[increment : (subbatch_size + increment)]\n",
    "                batch_labels_test = c10_test_labels[increment : (subbatch_size + increment)]\n",
    "                \n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "                                                                                        \n",
    "                print('Epoch # {}, CIFAR-10 Batch # {}, chunk = [{}:{}]  '.format(epoch + 1, batch_i,\n",
    "                                                                                   increment, (subbatch_size + increment)), end='')\n",
    "                increment += subbatch_size\n",
    "                \n",
    "                print_stats(sess, batch_features, batch_labels, cost, batch_features_test, \\\n",
    "                                                  batch_labels_test, merged_summary_op, summary_writer)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
